---
layout: page
title: MATH-AI
subtitle: "The 4th Workshop on Mathematical Reasoning and AI"
use-site-title: true
---
<div class="venue" style="font-size: 27px; display: block; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: 300; color: #404040; text-align: center;">
  (West Meeting Room 118-120, Vancouver, December 14, 2024, <a href="https://neurips.cc/virtual/2024/workshop/84719" target="_blank">Website</a>)
</div>

# Accepted Papers

<ol>
    <li>Semantic Self-Consistency: Enhancing Language Model Reasoning via Semantic Weighting</li>
    <li>Probabilistic Proof State Compression: Optimizing LLM-Guided Formal Verification</li>
    <li>Constraint-Based Synthetic Data Generation for LLM Mathematical Reasoning</li>
    <li>Synchronizing Verbal Responses and Board Writing for Multimodal Math Instruction with LLMs</li>
    <li>ABEL: Sample Efficient Online Reinforcement Learning for Neural Theorem Proving</li>
    <li>AI-Assisted Generation of Difficult Math Questions</li>
    <li>How Transformers Reason: A Case Study on a Synthetic Propositional Logic Problem</li>
    <li>Learning Elementary Cellular Automata with Transformers</li>
    <li>Math for AI: On the Generalization of Learning Mathematical Problem Solving</li>
    <li>Genetic Curriculum Learning for Distribution Generalization on the Travelling Salesman Problem</li>
    <li>Structure Based Dataset on SAT Solving with Graph Neural Networks</li>
    <li>A Hessian View of Grokking in Mathematical Reasoning</li>
    <li>Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning</li>
    <li>Generative Verifiers: Reward Modeling as Next-Token Prediction</li>
    <li>MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula</li>
    <li>Not All LLM Reasoners Are Created Equal</li>
    <li>Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically</li>
    <li>Machine Learning meets Algebraic Combinatorics: A Suite of Datasets to Accelerate AI for Mathematics Research</li>
    <li>Repeated examples help learn arithmetic</li>
    <li>VinePPO: Accurate Credit Assignment in RL for LLM Mathematical Reasoning</li>
    <li>Transformers to Predict the Applicability of Symbolic Integration Routines</li>
    <li>NLIR: Natural Language Intermediate Representation for Mechanized Theorem Proving</li>
    <li>DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images</li>
    <li>Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models</li>
    <li>Library Learning Doesn't: The Curious Case of the Single-Use "Library"</li>
    <li>On Memorization of Large Language Models in Logical Reasoning</li>
    <li>Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning</li>
    <li>MathDSL: A Domain-Specific Language for Concise Mathematical Solutions Via Program Synthesis</li>
    <li>Transformers Can Do Arithmetic with the Right Embeddings</li>
    <li>miniCTX: Neural Theorem Proving with (Long-)Contexts</li>
    <li>Mining Math Conjectures from LLMs: A Pruning Approach</li>
    <li>The Art of Knowing When to Stop: Analysis of Optimal Stopping in People and Machines</li>
    <li>The Karp Dataset</li>
    <li>Towards Faster Quantum Circuit Simulation Using Graph Decompositions, GNNs and Reinforcement Learning</li>
    <li>Intermediate Fine-Tuning Improves Mathematical Reasoning in Smaller Models</li>
    <li>Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data</li>
    <li>Interleaving Text and Number Embeddings to Solve Mathemathics Problems</li>
    <li>Looped Transformers for Length Generalization</li>
    <li>TurtleBench: A Visual Programming Benchmark in Turtle Geometry</li>
    <li>Wu's Method Boosts Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry</li>
    <li>InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning</li>
    <li>Models Can and Should Embrace the Communicative Nature of Human-Generated Math</li>
    <li>CausalBench: A Comprehensive Benchmark for Evaluating Causal Reasoning Capabilities of Large Language Models</li>
    <li>FEABench: Evaluating Language Models on Real World Physics Reasoning Ability</li>
    <li>Reasoning and Tools for Forecasting</li>
    <li>Reasoning in Reasoning: A Hierarchical Framework for Better and Faster Neural Theorem Proving</li>
    <li>CAFA: Coding as Auto-Formulation Can Boost Large Language Models in Solving Linear Programming Problem</li>
    <li>Synthesizing Verified Mathematical Problems</li>
    <li>LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery</li>
    <li>HARDMATH: A Benchmark Dataset for Challenging Problems in Applied Mathematics</li>
    <li>SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation</li>
    <li>Give me a hint: Can LLMs take a hint to solve math problems?</li>
    <li>SBSC: Step-by-Step Coding for Improving Mathematical Olympiad Performance</li>
    <li>Lean-STaR: Learning to Interleave Thinking and Proving</li>
    <li>Math2Sym: A System for Solving Elementary Problems via Large Language Models and Symbolic Solvers</li>
    <li>Skywork-Math: Data Scaling Laws for Mathematical Reasoning in LLMs — The Story Goes On</li>
    <li>Proving Olympiad Algebraic Inequalities without Human Demonstrations</li>
    <li>Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving</li>
    <li>STEM-PoM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing</li>
    <li>Machines and Mathematical Mutations: Using GNNs to Characterize Quiver Mutation Classes</li>
    <li>Attention Bias as an Inductive Bias: How to Teach Transformers Simple Arithmetic</li>
    <li>Learning Mathematical Rules with Large Language Models</li>
    <li>Formal Representation and Solution of Plane Geometric Problems</li>
    <li>OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data</li>
    <li>WILT: A Multi-turn, Memorization-Robust Inductive Logic Benchmark for LLMs</li>
    <li>Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</li>
    <li>VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search</li>
    <li>Regress, Don't Guess – A Regression-like Loss on Number Tokens for Language Models</li>
    <li>DafnyBench: A Benchmark for Formal Software Verification</li>
</ol>

The list of accepted papers can be found on OpenReview <a href="https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/MATH-AI#tab-accept">here</a>.

# Reviewers

We are grateful to our fantastic reviewers for making our workshop reviewing process run smoothly:

<div class="reviewers">
<ul>
{% for reviewer in site.data.pc.people %}
    <li>{{ reviewer }}</li>
{% endfor %}
</ul>
</div>

<style>
.reviewers ul {
    columns: 4;
    -webkit-columns: 4;
    -moz-columns: 4;
    list-style-position: inside;
    padding-left: 0;
}
.reviewers li {
    break-inside: avoid;
    page-break-inside: avoid;
    padding: 2px 0;
}
</style>